{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14665961,"sourceType":"datasetVersion","datasetId":9369375}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import basic libraries and show versions for debugging\nimport os                                         # access filesystem\nimport math                                       # math functions (exp)\nimport random                                     # random choices for sampling\nimport time                                       # measure elapsed time\nimport numpy as np                                # numeric arrays\nimport matplotlib.pyplot as plt                   # plotting\nimport torch                                      # main PyTorch package\nimport torch.nn as nn                             # neural network modules\nfrom torch.utils.data import Dataset, DataLoader  # dataset and dataloader utilities\n\n# print library versions and device info to confirm environment\nprint(\"numpy\", np.__version__)                     # show numpy version\nprint(\"torch\", torch.__version__)                  # show torch version\nprint(\"cuda available:\", torch.cuda.is_available())# show if GPU is available\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:02:15.701402Z","iopub.execute_input":"2026-01-29T16:02:15.701753Z","iopub.status.idle":"2026-01-29T16:02:15.707806Z","shell.execute_reply.started":"2026-01-29T16:02:15.701725Z","shell.execute_reply":"2026-01-29T16:02:15.707101Z"}},"outputs":[{"name":"stdout","text":"numpy 2.0.2\ntorch 2.8.0+cu126\ncuda available: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# path to your file; change if the file is in another folder\nfile_path = \"/kaggle/input/office-script-clean/office_script_clean.txt\"              # your provided script filename\n\n# read the text file into a string variable\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:   # open file for reading\n    raw_text = f.read()                             # read the whole file into memory\n\n# simple cleanup: normalize line endings to '\\n' (if needed)\nraw_text = raw_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")  # normalize newlines\n\n# report how many characters we loaded so you can confirm file read\nprint(\"Loaded characters:\", len(raw_text))          # print total characters loaded\n\n# preview the first 800 characters so you see formatting\nprint(raw_text[:800])                               # show a short preview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:08:54.350150Z","iopub.execute_input":"2026-01-29T16:08:54.350873Z","iopub.status.idle":"2026-01-29T16:08:54.499656Z","shell.execute_reply.started":"2026-01-29T16:08:54.350847Z","shell.execute_reply":"2026-01-29T16:08:54.499087Z"}},"outputs":[{"name":"stdout","text":"Loaded characters: 3427466\nMichael: All right Jim. Your quarterlies look very good. How are things at the library?\nJim: Oh, I told you. I couldn't close it. So...\nMichael: So you've come to the master for guidance? Is this what you're saying, grasshopper?\nJim: Actually, you called me in here, but yeah.\nMichael: All right. Well, let me show you how it's done.\nMichael:  Yes, I'd like to speak to your office manager, please. Yes, hello. This is Michael Scott. I am the Regional Manager of Dunder Mifflin Paper Products. Just wanted to talk to you manager-a-manger.  All right. Done deal. Thank you very much, sir. You're a gentleman and a scholar. Oh, I'm sorry. OK. I'm sorry. My mistake.  That was a woman I was talking to, so... She had a very low voice. Probably a smoker, so...  So that's the way it's done.\nMichael: I've\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# build character vocabulary (unique chars found in the dataset)\nvocab = sorted(list(set(raw_text)))                 # unique characters sorted for stability\n\n# build mappings: char -> id and id -> char\nstoi = {ch:i for i,ch in enumerate(vocab)}          # string to index mapping\nitos = {i:ch for i,ch in enumerate(vocab)}          # index to string mapping\n\n# encode entire raw_text into a numpy array of integer ids\ndata_ids = np.array([stoi[ch] for ch in raw_text], dtype=np.int64)  # numeric encoding\n\n# data statistics requested\nvocab_size = len(vocab)                             # number of unique characters\ntotal_chars = len(raw_text)                         # total characters in dataset\ncontext_length = 128                                # chosen context length (can be changed)\n\n# print the stats so you get the exact numbers for your dataset\nprint(\"vocab size:\", vocab_size)                    # print vocab size\nprint(\"total characters:\", total_chars)             # print total characters\nprint(\"context length (suggested):\", context_length)  # print context length choice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:09:07.328778Z","iopub.execute_input":"2026-01-29T16:09:07.329084Z","iopub.status.idle":"2026-01-29T16:09:07.661488Z","shell.execute_reply.started":"2026-01-29T16:09:07.329020Z","shell.execute_reply":"2026-01-29T16:09:07.660866Z"}},"outputs":[{"name":"stdout","text":"vocab size: 72\ntotal characters: 3427466\ncontext length (suggested): 128\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 90/10 train/validation split index\nsplit_at = int(0.9 * len(data_ids))                 # compute index for 90% split\n\n# split encoded ids into train and validation arrays\ntrain_ids = data_ids[:split_at]                     # training portion ids\nval_ids = data_ids[split_at:]                       # validation portion ids\n\n# define a simple PyTorch Dataset for sliding windows of characters\nclass CharDataset(Dataset):                          # subclass Dataset\n    def __init__(self, arr, context_len):            # constructor with array and window size\n        self.arr = arr                               # store array of ids\n        self.context_len = context_len               # store context length\n    def __len__(self):                               # required: number of samples\n        return max(0, len(self.arr) - self.context_len)  # number of windows\n    def __getitem__(self, idx):                      # required: get one sample by index\n        x = self.arr[idx: idx + self.context_len]   # input window of length context_len\n        y = self.arr[idx + 1: idx + 1 + self.context_len]  # target is next characters\n        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)  # return tensors\n\n# instantiate datasets for train and validation\ntrain_dataset = CharDataset(train_ids, context_length)  # training dataset instance\nval_dataset = CharDataset(val_ids, context_length)      # validation dataset instance\n\n# dataloader parameters\nbatch_size = 64                                     # batch size (tune for your GPU/CPU)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)  # train loader\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)     # val loader\n\n# show how many batches per epoch for sanity check\nprint(\"Train batches per epoch:\", len(train_loader))   # number of training batches\nprint(\"Val batches per epoch:\", len(val_loader))       # number of validation batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:10:06.567339Z","iopub.execute_input":"2026-01-29T16:10:06.567621Z","iopub.status.idle":"2026-01-29T16:10:06.575664Z","shell.execute_reply.started":"2026-01-29T16:10:06.567598Z","shell.execute_reply":"2026-01-29T16:10:06.575083Z"}},"outputs":[{"name":"stdout","text":"Train batches per epoch: 48196\nVal batches per epoch: 5353\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# tuned hyperparameters for the full script\nembed_size = 128                     # embedding vector size for each character\nhidden_size = 512                    # LSTM hidden dimension (512 recommended for quality)\nn_layers = 2                         # stacked LSTM layers\nlearning_rate = 0.0015               # Adam learning rate\nnum_epochs = 40                      # epochs to run\ncheckpoint_path = \"lstm_modules_full.pt\"  # where checkpoints will be saved\n\n# define modules and move them to the device\nembed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size).to(device)  # char -> vector\nlstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=n_layers, batch_first=True).to(device)  # recurrent core\nfc = nn.Linear(in_features=hidden_size, out_features=vocab_size).to(device)  # project LSTM outputs to vocab logits\n\n# collect parameters from modules for optimizer and gradient operations\nparams = list(embed.parameters()) + list(lstm.parameters()) + list(fc.parameters())  # gather all trainable params\noptimizer = torch.optim.Adam(params, lr=learning_rate)  # Adam optimizer over all params\ncriterion = nn.CrossEntropyLoss()                        # loss: cross-entropy over vocab logits\n\n# print a small sanity summary so you know the model & device state\nprint(\"Device:\", device)                                  # which device is being used\nprint(\"Vocab size:\", vocab_size)                          # number of unique characters\nprint(\"Parameter count:\", sum(p.numel() for p in params)) # total model parameters\nprint(\"Train batches per epoch:\", len(train_loader))      # how many train batches per epoch\nprint(\"Val batches per epoch:\", len(val_loader))          # how many val batches per epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:24:22.619449Z","iopub.execute_input":"2026-01-29T16:24:22.620082Z","iopub.status.idle":"2026-01-29T16:24:22.657901Z","shell.execute_reply.started":"2026-01-29T16:24:22.620027Z","shell.execute_reply":"2026-01-29T16:24:22.657297Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nVocab size: 72\nParameter count: 3462216\nTrain batches per epoch: 48196\nVal batches per epoch: 5353\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def evaluate_modules(embed, lstm, fc, dataloader):\n    \"\"\"\n    Compute average negative log-likelihood (NLL) and perplexity on dataloader.\n    Returns (avg_nll, perplexity).\n    \"\"\"\n    # set modules to eval mode (affects dropout/batchnorm if present)\n    embed.eval(); lstm.eval(); fc.eval()\n\n    total_loss = 0.0                 # accumulator for loss * token_count\n    total_tokens = 0                 # total token count seen\n\n    with torch.no_grad():            # turn off gradients for evaluation\n        for xb, yb in dataloader:    # iterate mini-batches\n            xb = xb.to(device)       # move input batch to device\n            yb = yb.to(device)       # move target batch to device\n\n            emb = embed(xb)          # (B, T, E) embeddings for the batch\n            out, _ = lstm(emb)       # (B, T, H) LSTM outputs (ignore hidden state)\n            logits = fc(out)         # (B, T, V) logits over vocabulary\n\n            B, T, V = logits.shape   # batch, time, vocab sizes\n            logits_flat = logits.view(B*T, V)     # flatten to (B*T, V) for loss\n            targets_flat = yb.view(B*T)          # flatten targets to (B*T)\n\n            loss = criterion(logits_flat, targets_flat)  # cross-entropy averaged over B*T\n            total_loss += loss.item() * (B*T)            # weight by token count\n            total_tokens += (B*T)                       # increase token counter\n\n    avg_nll = total_loss / total_tokens        # average negative log-likelihood per token\n    ppl = math.exp(avg_nll)                    # perplexity = exp(avg NLL)\n    return avg_nll, ppl                        # return both values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_modules_with_history(embed, lstm, fc, train_loader, val_loader, num_epochs, save_path):\n    \"\"\"\n    Train the embed/lstm/fc modules and return histories:\n    (train_losses, val_losses, train_ppls, val_ppls).\n    Checkpoints saved to save_path after each epoch.\n    \"\"\"\n    embed.to(device); lstm.to(device); fc.to(device)  # ensure modules are on correct device\n\n    train_losses = []      # store average train NLL per epoch\n    val_losses = []        # store average val NLL per epoch\n    train_ppls = []        # store train perplexities per epoch\n    val_ppls = []          # store val perplexities per epoch\n\n    for epoch in range(num_epochs):                   # epoch loop\n        embed.train(); lstm.train(); fc.train()       # set modules to training mode\n        running_loss = 0.0                            # accumulate train loss * tokens\n        running_tokens = 0                            # accumulate train tokens\n        t0 = time.time()                              # epoch start time\n\n        for xb, yb in train_loader:                   # iterate training batches\n            xb = xb.to(device); yb = yb.to(device)    # move tensors to device\n            optimizer.zero_grad()                     # zero gradients before backprop\n\n            emb_out = embed(xb)                       # (B, T, E) embeddings\n            seq_out, _ = lstm(emb_out)                # (B, T, H) LSTM outputs\n            logits = fc(seq_out)                      # (B, T, V) logits\n\n            B, T, V = logits.shape                    # get shapes for flattening\n            logits_flat = logits.view(B*T, V)         # flatten logits -> (B*T, V)\n            targets_flat = yb.view(B*T)               # flatten targets -> (B*T,)\n\n            loss = criterion(logits_flat, targets_flat)  # compute loss (scalar)\n            loss.backward()                            # backpropagate gradients\n            torch.nn.utils.clip_grad_norm_(params, 1.0) # clip gradient norm for stability\n            optimizer.step()                           # update model parameters\n\n            running_loss += loss.item() * (B*T)        # accumulate weighted loss\n            running_tokens += (B*T)                    # accumulate token count\n\n        # evaluate after finishing epoch to record metrics\n        train_loss, train_ppl = evaluate_modules(embed, lstm, fc, train_loader)  # train metrics\n        val_loss, val_ppl = evaluate_modules(embed, lstm, fc, val_loader)        # validation metrics\n\n        train_losses.append(train_loss)               # save train loss\n        val_losses.append(val_loss)                   # save val loss\n        train_ppls.append(train_ppl)                  # save train perplexity\n        val_ppls.append(val_ppl)                      # save val perplexity\n\n        t1 = time.time()                               # epoch end time\n        # print concise progress line with numbers you care about\n        print(f\"Epoch {epoch+1}/{num_epochs} | train_loss {train_loss:.4f} ppl {train_ppl:.3f} | val_loss {val_loss:.4f} ppl {val_ppl:.3f} | time {t1-t0:.1f}s\")\n\n        # checkpoint: save module state dicts and small metadata so you can resume or sample later\n        torch.save({\n            'embed_state': embed.state_dict(), 'lstm_state': lstm.state_dict(), 'fc_state': fc.state_dict(),\n            'stoi': stoi, 'itos': itos, 'epoch': epoch+1, 'train_loss': train_loss, 'val_loss': val_loss, 'val_ppl': val_ppl\n        }, save_path)\n\n    return train_losses, val_losses, train_ppls, val_ppls  # return recorded histories\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:24:56.020712Z","iopub.execute_input":"2026-01-29T16:24:56.020996Z","iopub.status.idle":"2026-01-29T16:24:56.031608Z","shell.execute_reply.started":"2026-01-29T16:24:56.020971Z","shell.execute_reply":"2026-01-29T16:24:56.030831Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def generate_script(seed_text, temperature, num_tokens_to_generate):\n    \"\"\"\n    Generate characters using saved LSTM modules if checkpoint exists.\n    If no checkpoint found, fall back to bigram sampling built from training text.\n    Signature required by the task: generate_script(seed_text, temperature, num_tokens_to_generate)\n    \"\"\"\n    ckpt = checkpoint_path  # uses the same checkpoint path defined earlier\n\n    if os.path.exists(ckpt):                           # if a trained checkpoint file exists\n        ck = torch.load(ckpt, map_location=device)     # load checkpoint to CPU/GPU as needed\n        embed.load_state_dict(ck['embed_state'])       # restore embedding weights\n        lstm.load_state_dict(ck['lstm_state'])         # restore LSTM weights\n        fc.load_state_dict(ck['fc_state'])             # restore final linear weights\n        embed.to(device); lstm.to(device); fc.to(device)  # ensure modules are on device\n        embed.eval(); lstm.eval(); fc.eval()           # set modules to evaluation mode\n\n        if len(seed_text) == 0:                         # if seed is empty, start with newline\n            seed_text = \"\\n\"\n\n        # map seed chars to ids using global stoi (unknown -> 0)\n        input_ids = [stoi.get(ch, 0) for ch in seed_text]\n        inp = torch.tensor([input_ids], dtype=torch.long).to(device)  # shape (1, L)\n        out_text = seed_text                              # initialize output string with seed\n\n        with torch.no_grad():                             # disable gradients during sampling\n            emb_seed = embed(inp)                         # embed the seed -> (1, L, E)\n            out_seed, hidden = lstm(emb_seed)             # run entire seed to prime hidden state\n            last_id = input_ids[-1]                       # start sampling conditioned on last seed id\n\n            for _ in range(num_tokens_to_generate):       # generate requested number of characters\n                step_inp = torch.tensor([[last_id]], dtype=torch.long).to(device)  # (1,1) tensor\n                step_emb = embed(step_inp)                # embedding for the last token -> (1,1,E)\n                logits_step, hidden = lstm(step_emb, hidden)  # one-step forward using current hidden\n                logits_np = logits_step[0, -1, :].cpu().numpy()  # extract logits for vocabulary (V,)\n\n                # temperature handling: low temperature -> deterministic, high -> more random\n                if temperature <= 0.0:\n                    temperature = 1e-8\n\n                logits_scaled = logits_np / temperature    # scale logits by 1/temperature\n                exps = np.exp(logits_scaled - np.max(logits_scaled))  # subtract max for stability\n                probs = exps / exps.sum()                 # normalized sampling probabilities\n\n                next_id = np.random.choice(len(probs), p=probs)  # sample next char id\n                next_char = itos[next_id]                   # convert id back to character\n                out_text += next_char                       # append to generated output\n                last_id = next_id                           # update last_id for next step\n\n        return out_text                                   # return generated text from LSTM\n\n    else:\n        # fallback: build (or reuse) bigram model for immediate sampling if no LSTM checkpoint\n        if 'bigram' not in globals():                    # if bigram not available, build it\n            # build bigram from training portion of raw_text\n            train_text = raw_text[:int(0.9 * len(raw_text))]    # same 90/10 split as before\n            chars = sorted(list(set(train_text)))               # bigram vocab\n            stoi_big = {ch:i for i,ch in enumerate(chars)}      # char->id for bigram\n            itos_big = {i:ch for i,ch in enumerate(chars)}      # id->char for bigram\n            V = len(chars)                                      # bigram vocab size\n            counts = np.zeros((V, V), dtype=np.float64)        # bigram counts\n            unigram = np.zeros(V, dtype=np.float64)            # unigram counts\n\n            for a, b in zip(train_text, train_text[1:]):       # count consecutive pairs\n                ai = stoi_big[a]; bi = stoi_big[b]\n                counts[ai, bi] += 1.0\n                unigram[ai] += 1.0\n            unigram[stoi_big[train_text[-1]]] += 1.0           # last char count\n            alpha = 1e-3\n            bigram_probs = (counts + alpha) / (counts.sum(axis=1, keepdims=True) + alpha * V)  # row-normalize\n            unigram_probs = (unigram + alpha) / (unigram.sum() + alpha * V)\n\n            # store globally for reuse during this session\n            bigram = {'chars': chars, 'stoi': stoi_big, 'itos': itos_big, 'bigram_probs': bigram_probs, 'unigram_probs': unigram_probs}\n\n        # perform sampling from the bigram model with temperature\n        if len(seed_text) == 0:\n            seed_text = \"\\n\"\n        out_text = seed_text                                  # initialize with the seed\n        last_char = seed_text[-1]                             # condition on last char of seed\n\n        for _ in range(num_tokens_to_generate):               # generate requested chars\n            if last_char in bigram['stoi']:                   # get conditional distribution\n                probs = bigram['bigram_probs'][bigram['stoi'][last_char]].copy()\n            else:\n                probs = bigram['unigram_probs'].copy()       # fallback to unigram after unknown char\n\n            if temperature <= 0.0:                            # guard temperature\n                temperature = 1e-8\n\n            logits = np.log(probs + 1e-12) / temperature      # apply temperature in log space\n            exps = np.exp(logits - np.max(logits))            # numerical stability subtraction\n            probs_t = exps / exps.sum()                       # normalized probabilities\n\n            next_i = np.random.choice(np.arange(len(probs_t)), p=probs_t)  # sample char index\n            next_c = bigram['itos'][next_i]                   # convert index to char\n            out_text += next_c                                # append generated char\n            last_char = next_c                                # update last_char for next step\n\n        return out_text                                      # return bigram-generated text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:25:07.502689Z","iopub.execute_input":"2026-01-29T16:25:07.503391Z","iopub.status.idle":"2026-01-29T16:25:07.516463Z","shell.execute_reply.started":"2026-01-29T16:25:07.503351Z","shell.execute_reply":"2026-01-29T16:25:07.515850Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#Train the model\nhist_train_losses, hist_val_losses, hist_train_ppls, hist_val_ppls = train_modules_with_history(\n    embed, lstm, fc, train_loader, val_loader, num_epochs=num_epochs, save_path=checkpoint_path\n)                                                      # trains modules and saves checkpoint each epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:25:52.139527Z","iopub.execute_input":"2026-01-29T16:25:52.140339Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/40 | train_loss 0.6826 ppl 1.979 | val_loss 1.6957 ppl 5.450 | time 4194.2s\nEpoch 2/40 | train_loss 0.7107 ppl 2.035 | val_loss 1.6527 ppl 5.221 | time 4189.6s\nEpoch 3/40 | train_loss 0.8593 ppl 2.361 | val_loss 1.4673 ppl 4.337 | time 4190.5s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}